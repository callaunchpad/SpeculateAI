{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to TensorFlow\n",
    "We will be fitting a sin(x) curve in this notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade numpy\n",
    "#!pip install tensorflow==1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Some Data To Fit\n",
    "**The first step in any good machine learning workflow is to get some data.** For this notebook we are just going to be fitting a sin curve with some Gaussian noise, so the data will be generated. The steps are then to\n",
    "1. Get the data by sampling data points in a sin(x) curve\n",
    "2. Add noise\n",
    "3. Build model\n",
    "4. Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a linear sampling space for the x values we will sample\n",
    "inputs = np.linspace(-5000, 5000, 100000)\n",
    "# Take the sin of the input \"x\" values to get our labels\n",
    "outputs = np.sin(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some of the data\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.axis([-5000, -4900, -2, 2])\n",
    "plt.plot(inputs[0:1000], outputs[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add noise to the data to make it realistic, data in the real world is **always** noisy. We are sampling noise from a normal (Gaussian) distribution with low variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_deviation = 1e-1 # The standard deviation of the noise distribution we want\n",
    "num_samples = inputs.shape[0]\n",
    "# Sample some random noise\n",
    "random_noise = np.random.normal(loc=0, scale=standard_deviation, size=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the noise to our inputs\n",
    "noisy_outputs = random_noise + outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(inputs[1:1000], random_noise[1:1000])\n",
    "plt.show()\n",
    "\n",
    "# Plot the data with noise\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(inputs[1:1000], noisy_outputs[1:1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have our noisy data, lets try to fit this with a deep neural network using TensorFlow. <br/>\n",
    "**Note:** In reality you'd probably never use a neural net for this, you'd use linear regression but that's not why we're here ¯\\\\_(ツ)_/¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief TLDR; on TensorfFlow\n",
    "TensorFlow is a useful Python library that we often use in deep learning. TensorFlow is intentionally built very generically to allow users to do pretty much anything with their models. Although TensorFlow is thought of as a deep learning library its core features are fairly basic. That is, TensorFlow can<br/>\n",
    "1. Build arbitrarily complicated computation graphs.\n",
    "2. Take derivatives/gradients through this graph which allows us to (for example) train models.\n",
    "<br/><br/>\n",
    "\n",
    "All of Tensorflow essentially boils down to building a computation graph and then optimizing it. So first **what is a computation graph?** A computation graph is just a **directed graph** (remember CS70) where the vertices of the graph are **values called tensors** (numbers, matrices, vectors) or **operations** (multiplication, addition, concatenation, convolution). Since we can build all of the operations in mathematics from a few fundamental ones, a computation graph can completely express any function. **Below** is an example of the formula $\\frac{ab}{a + b}$ evaluated for $(a, b) = (15, 5)$.\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/2994/1*vPb9E0Yd1QUAD0oFmAgaOw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does TensorFlow handle computation graphs?**<br/>\n",
    "Computation graphs in TF are built on a few core concepts<br/>\n",
    "- [Constants](https://www.tensorflow.org/api_docs/python/tf/constant) `tf.constant(value, dtype=None, shape=None, name='Const')`: holds constants like pi or acceleration due to gravity.\n",
    "- [Placeholders](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/placeholder) `tf.placeholder(dtype, shape=None, name=None)`: graph node for something we will pass in (e.g. inputs to a model or labels to train on)\n",
    "- [Variables](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/Variable) `tf.Variable(initial_value=None, shape=None, trainable=None)`: holds variables that you may train in the graph or may change throughout computation.\n",
    "- [Operations](https://www.tensorflow.org/api_docs/python/tf/Operation): means of operating and transforming the above three graph elements and more.\n",
    "<br/><br/>\n",
    "\n",
    "These fundamental core objects (along with a few others) make up the majority of all TF graphs. **Note:** Tensorflow calls all these intermediate values (constants, placeholders, variables) **tensors** and tracks maintains data on them as well as outputs of operations as they **flow** through the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning models are often incredibly complex and it would be a pain to have to build these models from scratch every time we want to use them, so the software engineers at Google built another level of abstraction into TensorFlow called the **layers** API. This API [linked here](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/layers) is incredibly useful and defines all the variables and computational elements for you for most things you will ever need to do. **Note:** TF still gives you the ability to define new layers if you so wish. Lets see an example, suppose we wanted to build a convolutional neural network. Instead of defining the [convolution](https://en.wikipedia.org/wiki/Convolution) operation ourselves in terms of addition, multiplication, and complicated indexing, we can just call `tf.layers.conv2d(inputs, filters, kernel_size, ...)` as per the [documentation](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/layers/conv2d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to build a two layer neural network where each layer is a **fully connected** layer (the simplest NN layer that is really just a matrix multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the computation graph in TF so we can build a new one\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First:** We want to define our placeholders (tensors that initially have no value but where we will store our inputs and labels in the graph). Use the documentation [here](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/placeholder) and data type `tf.float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build input and output placeholders\n",
    "graph_input = # YOUR CODE HERE\n",
    "graph_labels = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second:** Add three fully connected (dense) layers to the network as per [this link](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/layers/Dense) store the output of these three layers (our network) in a variable called **output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "output = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third:** Define a loss for our data, use [mean squared error](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally:** We define an optimizer and a tensorflow operation called `train_op` that will be called to make a training step on our network. I've done this part for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3 # The learning rate we will use for training\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss) # Calling this operation will make a training step on the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a TF session to evaluate our graph in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "inputs = inputs.reshape((-1, 1))\n",
    "noisy_outputs = noisy_outputs.reshape((-1, 1))\n",
    "losses = []\n",
    "\n",
    "# Run training script\n",
    "for i in range(epochs):\n",
    "    feed_dict = {\n",
    "        graph_input: inputs,\n",
    "        graph_labels: noisy_outputs\n",
    "    }\n",
    "    \n",
    "    this_loss, _ = sess.run((loss, train_op), feed_dict=feed_dict)\n",
    "    losses.append(this_loss)\n",
    "    \n",
    "    if i % 50 == 0 or i == epochs - 1:\n",
    "        print(f\"Loss on epoch {i} was: {this_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
